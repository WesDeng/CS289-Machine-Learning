{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to install \"gprof2dot\"\n",
    "import io\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from statistics import mode\n",
    "import sklearn.model_selection\n",
    "import sklearn.tree\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import pydot\n",
    "import multiprocessing as mp\n",
    "\n",
    "eps = 1e-5  # a small number\n",
    "\n",
    "\n",
    "# Data Processing for Titanic.\n",
    "from sklearn.preprocessing import LabelBinarizer, PolynomialFeatures\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# For Spam Kaggle Competition.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function.\n",
    "def results_to_csv(y_test, name):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1  # Ensures that the index starts at 1. \n",
    "    df.to_csv(name + '.csv', index_label='Id')\n",
    "def split(data, labels, val_size):\n",
    "    num_items = len(data)\n",
    "    assert num_items == len(labels)\n",
    "    assert val_size >= 0\n",
    "    if val_size < 1.0:\n",
    "        val_size = int(num_items * val_size)\n",
    "    train_size = num_items - val_size\n",
    "    idx = random.permutation(num_items)\n",
    "    data_train = data[idx][:train_size]\n",
    "    label_train = labels[idx][:train_size]\n",
    "    data_val = data[idx][train_size:]\n",
    "    label_val = labels[idx][train_size:]\n",
    "    return data_train, data_val, label_train, label_val\n",
    "\n",
    "def get_vote_result(vote):\n",
    "    cutoff = len(vote)//2\n",
    "    if sum(vote) > cutoff:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3.6>**3.1 Implement Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def gini_impurity(y):\n",
    "    # TODO implement gini_impurity function\n",
    "    labels = np.unique(y)\n",
    "    total_num = float(len(y))\n",
    "    gini_sum = 0\n",
    "        \n",
    "    for item in labels:\n",
    "        gini_sum += (sum([1 for i in list(y) if i == item])/total_num)**2\n",
    "            \n",
    "    return 1 - gini_sum\n",
    "\n",
    "\n",
    "def information_gain(X, y, thresh):\n",
    "\n",
    "    right_index = (X <= thresh)\n",
    "    left_index = (X > thresh)\n",
    "    y_right = np.extract(right_index, y)\n",
    "    y_left = np.extract(left_index, y)\n",
    "        \n",
    "    # Number of \n",
    "    n_parent = float(len(y))\n",
    "    n_left = float(len(y_left))\n",
    "    n_right = float(len(y_right))\n",
    "    assert n_parent == n_left + n_right, \"Wrong split\"\n",
    "        \n",
    "    I_parent = gini_impurity(y)\n",
    "    I_children = (n_left/n_parent) * gini_impurity(y_left) + \\\n",
    "                    (n_right/n_parent) * gini_impurity(y_right)\n",
    "    return I_parent - I_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self, max_depth=3, feature_labels=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # split_rule\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "   \n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh): \n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.max_depth > 0:\n",
    "            # compute entropy gain for all single-dimension splits,\n",
    "            # thresholding with a linear interpolation of 10 values\n",
    "            gains = []\n",
    "            # The following logic prevents thresholding on exactly the minimum\n",
    "            # or maximum values, which may not lead to any meaningful node\n",
    "            # splits.\n",
    "            thresh = np.array([\n",
    "                np.linspace(np.min(X[:, i]) + eps, np.max(X[:, i]) - eps, num=15)\n",
    "                for i in range(X.shape[1])\n",
    "            ])\n",
    "            for i in range(X.shape[1]):\n",
    "                gains.append([information_gain(X[:, i], y, t) for t in thresh[i, :]])\n",
    "\n",
    "            gains = np.nan_to_num(np.array(gains))\n",
    "            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains), gains.shape)\n",
    "            self.thresh = thresh[self.split_idx, thresh_idx]\n",
    "            X0, y0, X1, y1 = self.split(X, y, idx=self.split_idx, thresh=self.thresh)\n",
    "            if X0.size > 0 and X1.size > 0:\n",
    "                self.left = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.left.fit(X0, y0)\n",
    "                self.right = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.right.fit(X1, y1)\n",
    "            else:\n",
    "                self.max_depth = 0\n",
    "                self.data, self.labels = X, y\n",
    "                self.pred = stats.mode(y).mode[0]\n",
    "        else:\n",
    "            self.data, self.labels = X, y\n",
    "            self.pred = stats.mode(y).mode[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, track=False):\n",
    "        if X.ndim == 1:\n",
    "            X = np.reshape(X, [1, len(X)])\n",
    "        if self.max_depth == 0:\n",
    "            return self.pred * np.ones(X.shape[0])\n",
    "        else:\n",
    "            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx, thresh=self.thresh)\n",
    "            yhat = np.zeros(X.shape[0])\n",
    "            \n",
    "            if track:\n",
    "                word = spam_features[self.split_idx]\n",
    "                if X[:, self.split_idx] < self.thresh:\n",
    "                    print(word + \" < \", self.thresh)\n",
    "                    yhat[idx0] = self.left.predict(X0, track=track)\n",
    "                else:\n",
    "                    print(word + \" >= \", self.thresh)\n",
    "                    yhat[idx1] = self.right.predict(X1, track=track)\n",
    "            else:\n",
    "                yhat[idx0] = self.left.predict(X0)\n",
    "                yhat[idx1] = self.right.predict(X1)\n",
    "\n",
    "            return yhat\n",
    "        \n",
    "    def accuracy(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(yhat == y) / float(len(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>**3.2 Implement Random Forests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \n",
    "    def __init__(self, num_trees=99, max_depth=5, feature_labels=None):\n",
    "        self.num_trees = num_trees\n",
    "        #self.num_sample = num_sample\n",
    "        self.max_depth = max_depth\n",
    "        self.feature_labels = feature_labels\n",
    "        #self.num_feature = None\n",
    "        \n",
    "        self.trees = []\n",
    "        \n",
    "        for _ in range(num_trees):\n",
    "            \n",
    "            num_feature = int(np.sqrt(len(feature_labels)))\n",
    "            random_feature = np.random.choice(feature_labels, num_feature)\n",
    "            self.trees.append(DecisionTree(max_depth, random_feature))\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        num_sample = int(X.shape[0]*(2/3))\n",
    "        \n",
    "        pool = mp.Pool()\n",
    "        results = np.zeros(self.num_trees, dtype=object)\n",
    "        for i, dt in enumerate(self.trees):\n",
    "            idx = np.random.choice(range(X.shape[0]), num_sample)\n",
    "            sub_X = X[idx, :]\n",
    "            sub_y = y[idx]\n",
    "            #dt.fit(sub_X, sub_y)\n",
    "            results[i] = pool.apply_async(dt.fit, args=(sub_X, sub_y))\n",
    "            if verbose:\n",
    "                if i%20 == 0:\n",
    "                    print('%d Tree finish fitting' % i)\n",
    "        self.trees = [t.get() for t in results]\n",
    "        pool.close()\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def predict(self, X, verbose=True):\n",
    "        N = X.shape[0]\n",
    "        pred = np.zeros(N)\n",
    "        \n",
    "        for i in range(N):\n",
    "            votes = np.zeros(self.num_trees)\n",
    "            for j, dt in enumerate(self.trees):\n",
    "                votes[j] = dt.predict(X[i, :])\n",
    "            pred[i] = get_vote_result(votes)\n",
    "        return pred\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(yhat == y) / float(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Processing for Titanic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    # Temporarily assign -1 to missing data\n",
    "    data[data == b''] = '-1'\n",
    "\n",
    "    # Hash the columns (used for handling strings)\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term in counter.most_common():\n",
    "            if term[0] == b'-1':\n",
    "                continue\n",
    "            if term[-1] <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term[0])\n",
    "            onehot_encoding.append((data[:, col] == term[0]).astype(np.float))\n",
    "        data[:, col] = '0'\n",
    "    onehot_encoding = np.array(onehot_encoding).T\n",
    "    data = np.hstack([np.array(data, dtype=np.float), np.array(onehot_encoding)])\n",
    "\n",
    "    # Replace missing data with the mode value. We use the mode instead of\n",
    "    # the mean or median because this makes more sense for categorical\n",
    "    # features such as gender or cabin type, which are not ordered.\n",
    "    if fill_mode:\n",
    "        for i in range(data.shape[-1]):\n",
    "            mode = stats.mode(data[((data[:, i] < -1 - eps) +\n",
    "                                    (data[:, i] > -1 + eps))][:, i]).mode[0]\n",
    "            data[(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)][:, i] = mode\n",
    "\n",
    "    return data, onehot_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Part (b): preprocessing the titanic dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesley/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  \n",
      "/Users/wesley/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "path_train = './titanic_training.csv'\n",
    "data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
    "path_test = './titanic_testing_data.csv'\n",
    "test_data = genfromtxt(path_test, delimiter=',', dtype=None)\n",
    "y = data[1:, 0]  # label = survived\n",
    "class_names = [\"Died\", \"Survived\"]\n",
    "\n",
    "labeled_idx = np.where(y != b'')[0]\n",
    "y = np.array(y[labeled_idx], dtype=np.int)\n",
    "print(\"\\n\\nPart (b): preprocessing the titanic dataset\")\n",
    "X, onehot_features = preprocess(data[1:, 1:], onehot_cols=[1, 5, 7, 8])\n",
    "X = X[labeled_idx, :]\n",
    "Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "assert X.shape[1] == Z.shape[1]\n",
    "features = list(data[0, 1:]) + onehot_features\n",
    "titanic_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = './spam_data.mat'\n",
    "data = scipy.io.loadmat(path_train)\n",
    "SH_train = data['training_data']\n",
    "SH_y = np.squeeze(data['training_labels'])\n",
    "SH_test = data['test_data']\n",
    "class_names = [\"Ham\", \"Spam\"]\n",
    "spam_features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\", \"creative\",\n",
    "            \"height\", \"featured\", \"differ\", \"width\", \"other\", \"energy\", \"business\", \"message\",\n",
    "            \"volumes\", \"revision\", \"path\", \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\", \"square_bracket\",\n",
    "            \"ampersand\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5857"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SH_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 199 4138 1034\n"
     ]
    }
   ],
   "source": [
    "random = np.random.RandomState(66)\n",
    "\n",
    "t_X_train, t_X_val, t_y_train, t_y_val = split(X, y, 0.2)\n",
    "\n",
    "s_X_train, s_X_val, s_y_train, s_y_val = split(SH_train, SH_y, 0.2)\n",
    "\n",
    "print(len(t_X_train), len(t_X_val), len(s_X_train), len(s_X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4.2> **3.3 Describe Implementation details**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you deal with categorical features and missing values?\n",
    "\n",
    "<font color='blue'>One hot encoding and replace the missing values by mode and mean, depending on the situation. For example, for missing age value, I repalce them by getting the mean value from each pclass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was your stopping criterion?\n",
    "\n",
    "<font color='blue'>The maximum depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you implement random forests?</font>\n",
    "\n",
    "<font color='blue'>I build a list to hold the decision trees I need and iterate through all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you do anything special to speed up training?</font>\n",
    "\n",
    "<font color='blue'>Train the trees in parallel by importing multiprocessing library and call mp.Pool and pool.apply_async. The training speed is significantly faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything else cool you implemented?</font>\n",
    "\n",
    "<font color='blue'>I have a couple different verbose and track options for debugging and tracking the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>For Titanic's Kaggle competition, I combined SibSp and Parch to make a new feature.\n",
    "I also add the length of the ticket to be a new feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4> **3.4 Performance Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3.8> \n",
    "I choose random forests with 50 decision trees and 5 max depth. <br><br> Notice that if I change the depth to 15, the training accuracy for decision tree will be much higher(0.95) but the validation accuracy won't be too much higher, indicating overfitting.<br><br>Larger depth and tree numbers could lead more significant different in the validation accuracy.<br><hr>\n",
    "<font color=blue>**Titanic**</font>:<br><br> **Decision Tree**: &nbsp; &nbsp;&nbsp; train = 0.83, val = 0.798\n",
    "    <br><br> **Random Forests**: &nbsp;train = 0.83375, val = 0.807<br><br>\n",
    "<font color=blue>**SPAM**</font>:<br><br> **Decision Tree**:  &nbsp; &nbsp;&nbsp;  train = 0.811, val = 0.807\n",
    "    <br><br> **Random Forests**: &nbsp;train = 0.819, val = 0.8105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic DT train:  0.83 val: 0.7989949748743719\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTree(max_depth=5, feature_labels=titanic_features)\n",
    "dt.fit(t_X_train, t_y_train)\n",
    "train_acc = dt.accuracy(t_X_train,  t_y_train)\n",
    "val_acc = dt.accuracy(t_X_val,  t_y_val)\n",
    "print(\"Titanic DT train: \",train_acc, \"val:\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAM DT train:  0.8115031416143065 val: 0.8075435203094777\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTree(max_depth=5, feature_labels=spam_features)\n",
    "dt.fit(s_X_train, s_y_train)\n",
    "train_acc = dt.accuracy(s_X_train,  s_y_train)\n",
    "val_acc = dt.accuracy(s_X_val,  s_y_val)\n",
    "print(\"SPAM DT train: \",train_acc, \"val:\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Tree finish fitting\n",
      "20 Tree finish fitting\n",
      "40 Tree finish fitting\n",
      "Titanic RF train:  0.83375 val: 0.8140703517587939\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForest(num_trees=50, max_depth=5, feature_labels=titanic_features)\n",
    "rf.fit(t_X_train, t_y_train)\n",
    "train_acc = rf.accuracy(t_X_train,  t_y_train)\n",
    "val_acc = rf.accuracy(t_X_val,  t_y_val)\n",
    "print(\"Titanic RF train: \",train_acc, \"val:\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Tree finish fitting\n",
      "20 Tree finish fitting\n",
      "40 Tree finish fitting\n",
      "SPAM RF train:  0.8195364910584824 val: 0.8105364910584824\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForest(num_trees=50, max_depth=5, feature_labels=spam_features)\n",
    "rf.fit(s_X_train, s_y_train)\n",
    "train_acc = rf.accuracy(s_X_train,  s_y_train)\n",
    "val_acc = rf.accuracy(s_X_val,  s_y_val)\n",
    "print(\"SPAM RF train: \",train_acc, \"val:\", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>**3.5 Generating accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3.6>**1. (optional)**<br>\n",
    "**2. State the split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exclamation <  1e-05<br>\n",
    "meter <  1e-05<br>\n",
    "parenthesis >=  1e-05<br>\n",
    "dollar <  1e-05<br>\n",
    "featured <  1e-05<br>\n",
    "money <  1e-05<br>\n",
    "Hence the email is a <font color='blue'>ham <font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exclamation >=  1e-05<br>\n",
    "ampersand <  1e-05<br>\n",
    "meter <  1e-05<br>\n",
    "volumes <  1e-05<br>\n",
    "money <  1e-05<br>\n",
    "dollar <  1e-05<br>\n",
    "Hence the email is a <font color='red'>spam<font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclamation <  1e-05\n",
      "meter <  1e-05\n",
      "parenthesis >=  1e-05\n",
      "dollar <  1e-05\n",
      "featured <  1e-05\n",
      "money <  1e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTree(max_depth=6, feature_labels=spam_features)\n",
    "dt.fit(s_X_train, s_y_train);\n",
    "dt.predict(s_X_train[0], track=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclamation >=  1e-05\n",
      "ampersand <  1e-05\n",
      "meter <  1e-05\n",
      "volumes <  1e-05\n",
      "money <  1e-05\n",
      "dollar <  1e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.predict(s_X_train[6], track=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3.6>**3. Plotting the trend**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3.6> The validation accuracy start to rapidly grow from depth 1-5, and gradually reach the peak value at depth 26, with validation accuracy being 0.825. The best validation point is highlighted on the graph using a red spot.<br>See below for the code and the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 10\n",
      "done 20\n",
      "done 30\n",
      "done 40\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "depths = [i+1 for i in range(40)]\n",
    "train_acc_list, val_acc_list = [], []\n",
    "\n",
    "for i in depths:\n",
    "    dt = DecisionTree(max_depth=i, feature_labels=spam_features)\n",
    "    dt.fit(s_X_train, s_y_train)\n",
    "    train_acc = dt.accuracy(s_X_train,  s_y_train)\n",
    "    val_acc = dt.accuracy(s_X_val,  s_y_val)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "    if i%10 == 0:\n",
    "        print('done %d'% i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.figure(figsize=(8, 8))\n",
    "plt.plot(depths, train_acc_list, label='train')\n",
    "plt.plot(depths, val_acc_list, label='val')\n",
    "\n",
    "# Get the highest point\n",
    "ymax = max(val_acc_list)\n",
    "xpos = val_acc_list.index(ymax)\n",
    "xmax = depths[xpos]\n",
    "\n",
    "plt.plot(xmax, ymax, 'ro')\n",
    "plt.legend(loc='best')\n",
    "plt.title('train/val accuracy')\n",
    "plt.xlabel('Maximum depths')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 0.8249516441005803\n"
     ]
    }
   ],
   "source": [
    "print(xmax, ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>**Kaggle for Titanic using Pandas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA for Titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./titanic_training.csv')\n",
    "test = pd.read_csv('./titanic_testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>349255</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A/5 21172</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>male</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SC/AH 29037</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>male</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>C.A. 2673</td>\n",
       "      <td>20.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass   sex   age  sibsp  parch       ticket     fare cabin  \\\n",
       "432       0.0     3.0  male   NaN    0.0    0.0       349255   7.8958   NaN   \n",
       "775       0.0     3.0  male  32.0    0.0    0.0       370376   7.7500   NaN   \n",
       "643       0.0     3.0  male  22.0    0.0    0.0    A/5 21172   7.2500   NaN   \n",
       "838       0.0     2.0  male  37.0    1.0    0.0  SC/AH 29037  26.0000   NaN   \n",
       "256       0.0     3.0  male  13.0    0.0    2.0    C.A. 2673  20.2500   NaN   \n",
       "\n",
       "    embarked  \n",
       "432        C  \n",
       "775        Q  \n",
       "643        S  \n",
       "838        S  \n",
       "256        S  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex\n",
       "female    0.750716\n",
       "male      0.190769\n",
       "Name: survived, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['survived'].groupby(train['sex']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     448\n",
       "5     161\n",
       "4     109\n",
       "8      85\n",
       "10     56\n",
       "7      30\n",
       "9      28\n",
       "13     16\n",
       "17     14\n",
       "11     14\n",
       "18     11\n",
       "15      8\n",
       "12      8\n",
       "16      7\n",
       "3       5\n",
       "Name: ticket_len, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['ticket_len'] = train['ticket'].apply(lambda x: len(str(x)))\n",
    "train['ticket_len'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticket_first\n",
       "1    0.603550\n",
       "2    0.456311\n",
       "3    0.259146\n",
       "4    0.166667\n",
       "5    0.000000\n",
       "6    0.111111\n",
       "7    0.250000\n",
       "8    0.000000\n",
       "9    0.000000\n",
       "A    0.083333\n",
       "C    0.358209\n",
       "F    0.600000\n",
       "L    0.250000\n",
       "P    0.617647\n",
       "S    0.319444\n",
       "W    0.285714\n",
       "n         NaN\n",
       "Name: survived, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['ticket_first'] = train['ticket'].apply(lambda x: str(x)[0])\n",
    "train.groupby(['ticket_first'])['survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train['ticket'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticket_last\n",
       "0    0.414141\n",
       "1    0.369369\n",
       "2    0.397959\n",
       "3    0.388889\n",
       "4    0.279070\n",
       "5    0.340426\n",
       "6    0.438202\n",
       "7    0.340659\n",
       "8    0.538462\n",
       "9    0.340206\n",
       "E    0.250000\n",
       "n         NaN\n",
       "Name: survived, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['ticket_last'] = train['ticket'].apply(lambda x: str(x)[-1])\n",
    "train.groupby(['ticket_last'])['survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['ticket_last'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cabin_first\n",
       "A    0.500000\n",
       "B    0.714286\n",
       "C    0.611111\n",
       "D    0.682927\n",
       "E    0.724138\n",
       "F    0.600000\n",
       "G    0.750000\n",
       "T    0.000000\n",
       "n    0.308290\n",
       "Name: survived, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['cabin_first'] = train['cabin'].apply(lambda x: str(x)[0])\n",
    "train.groupby(['cabin_first'])['survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering based on\n",
    "# https://www.kaggle.com/zlatankr/titanic-random-forest-82-78#Ticket\n",
    "\n",
    "# Based on my own EDA with my own feature engineering procedure.\n",
    "def age_impute(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['Age_Null_Flag'] = i['age'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "        data = train.groupby(['pclass'])['age']\n",
    "        i['age'] = data.transform(lambda x: x.fillna(x.mean()))\n",
    "    return train, test\n",
    "\n",
    "def fam_size(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['fam_size'] = np.where((i['sibsp']+i['parch']) == 0 , 'Solo',\n",
    "                           np.where((i['sibsp']+i['parch']) <= 3,'Nuclear', 'Big'))\n",
    "        del i['sibsp']\n",
    "        del i['parch']\n",
    "    return train, test\n",
    "\n",
    "def ticket_grouped(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['ticket_first'] = i['ticket'].apply(lambda x: str(x)[0])\n",
    "        i['ticket_first'] = i['ticket_first'].apply(lambda x: str(x))\n",
    "        i['ticket_first'] = np.where((i['ticket_first']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), i['ticket_first'],\n",
    "                                   np.where((i['ticket_first']).isin(['W', '4', '7', '6', 'L', '5', '8','n']),\n",
    "                                            'Low_ticket', 'Other_ticket'))\n",
    "        i['ticket_len'] = i['ticket'].apply(lambda x: len(str(x)))\n",
    "        del i['ticket']\n",
    "    return train, test\n",
    "\n",
    "def cabin_first(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['cabin_first'] = i['cabin'].apply(lambda x: str(x)[0])\n",
    "        del i['cabin']\n",
    "    return train, test\n",
    "\n",
    "def embarked_impute(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['embarked'] = i['embarked'].fillna('S')\n",
    "    return train, test\n",
    "\n",
    "# This function is largely from https://www.kaggle.com/zlatankr/titanic-random-forest-82-78#Ticket\n",
    "def dummies(train, test, columns = ['pclass', 'sex', 'embarked', 'ticket_first', 'cabin_first', 'fam_size']):\n",
    "    for column in columns:\n",
    "        train[column] = train[column].apply(lambda x: str(x))\n",
    "        test[column] = test[column].apply(lambda x: str(x))\n",
    "        good_cols = [column+'_'+i for i in train[column].unique() if i in test[column].unique()]\n",
    "        train = pd.concat((train, pd.get_dummies(train[column], prefix = column)[good_cols]), axis = 1)\n",
    "        test = pd.concat((test, pd.get_dummies(test[column], prefix = column)[good_cols]), axis = 1)\n",
    "        del train[column]\n",
    "        del test[column]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing's completely my own work.\n",
    "train = pd.read_csv('./titanic_training.csv')\n",
    "train = train[train['survived'].notna()]\n",
    "test = pd.read_csv('./titanic_testing_data.csv')\n",
    "train['pclass'].fillna(train['pclass'].value_counts().idxmax(), inplace=True)\n",
    "train['fare'].fillna(train['fare'].mean(), inplace=True)\n",
    "train, test = age_impute(train, test)\n",
    "train, test = cabin_first(train, test)\n",
    "train, test = embarked_impute(train, test)\n",
    "train, test = fam_size(train, test)\n",
    "test['fare'].fillna(train['fare'].mean(), inplace=True)\n",
    "train, test = ticket_grouped(train, test)\n",
    "train, test = dummies(train, test, columns = \n",
    "                      ['pclass', 'sex', 'embarked', 'ticket_first', 'cabin_first', 'fam_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = train.iloc[:, 1:], train.iloc[:, 0]\n",
    "test_X = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coare tune.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n",
    "\n",
    "param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10], \"min_samples_split\" : [2, 4, 10, 12, 16], \"n_estimators\": [50, 100, 400, 700, 1000]}\n",
    "\n",
    "gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(train_X, train_y)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "print(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8108108108108109\n",
      "{'criterion': 'gini', 'min_samples_leaf': 1, 'min_samples_split': 12, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Fine tune.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n",
    "\n",
    "param_grid = { \"criterion\" : [\"gini\"], \"min_samples_leaf\" : [1, 5], \"min_samples_split\" : [10, 11, 12], \"n_estimators\": [40,50,60,100]}\n",
    "\n",
    "gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(train_X, train_y)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8108108108108109\n",
      "{'criterion': 'gini', 'min_samples_leaf': 1, 'min_samples_split': 12, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Fine tune.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n",
    "\n",
    "param_grid = { \"criterion\" : [\"gini\"], \"min_samples_leaf\" : [1, 5], \"min_samples_split\" : [10, 11, 12, 13, 14], \"n_estimators\": [48,49,50,51,52,53]}\n",
    "\n",
    "gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(train_X, train_y)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "titanic_kaggle_rf = RandomForestClassifier(criterion='gini',min_samples_leaf=1, max_depth=20,\n",
    "                                           min_samples_split=12, n_estimators=300, oob_score=True,\n",
    "                                          random_state=1,n_jobs=-1)\n",
    "titanic_kaggle_rf.fit(train_X, train_y)\n",
    "print(\"%.4f\" % titanic_kaggle_rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8828828828828829"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "titanic_kaggle_rf = RandomForestClassifier(criterion='gini',min_samples_leaf=1, \n",
    "                                           min_samples_split=12, n_estimators=300, oob_score=False,\n",
    "                                          random_state=1,n_jobs=-1)\n",
    "titanic_kaggle_rf.fit(train_X, train_y)\n",
    "titanic_kaggle_rf.score(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_pred = titanic_kaggle_rf.predict(test_X)\n",
    "results_to_csv(titanic_pred, 'titanic_kaggle_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>**Kaggle for SPAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import glob\n",
    "\n",
    "sf = glob.glob('data/spam/'  + '*.txt')\n",
    "hf = glob.glob('data/ham/' + '*.txt')\n",
    "tf = ['data/test/'  + str(x) + '.txt' for x in range(5857)]\n",
    "\n",
    "all_train, all_test = [], []\n",
    "\n",
    "for file in sf + hf:\n",
    "    with open(file, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        all_train.append(f.read())\n",
    "\n",
    "for file in tf:\n",
    "    with open(file, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        all_test.append(f.read())\n",
    "        \n",
    "# Contrast Normalization Helper funtion.\n",
    "def cn(X):\n",
    "    norm_X = np.zeros(X.shape)\n",
    "    for i in range(X.shape[0]):\n",
    "        norm_X[i, :] = X[i, :]/(np.linalg.norm(X[i, :])+1e-12)\n",
    "    return norm_X\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=4)\n",
    "raw_training_X = vectorizer.fit_transform(all_train)\n",
    "raw_test_X = vectorizer.transform(all_test)\n",
    "# Normalization.\n",
    "training_X, test_X = cn(raw_training_X.toarray()), cn(raw_test_X.toarray())\n",
    "training_y = np.concatenate((np.ones(len(sf)), np.zeros(len(hf))))\n",
    "\n",
    "std = np.std(training_X, axis=0)\n",
    "word_index = std.argsort()[-50:] # Tried out 600, 1000 and 1200.\n",
    "SH_train = training_X[:, word_index]\n",
    "SH_y = training_y\n",
    "SH_test = test_X[:, word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8288863109048724\n",
      "{'criterion': 'gini', 'max_depth': 25, 'min_samples_leaf': 1, 'min_samples_split': 11, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n",
    "\n",
    "param_grid = { \"criterion\" : [\"gini\"], \"max_depth\" : [20, 22, 25], \"min_samples_leaf\" : [1], \"min_samples_split\" : [8, 9, 10, 11, 12], \"n_estimators\": [50, 100, 200]}\n",
    "\n",
    "gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(SH_train, SH_y)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8413\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "spam_kaggle_rf = RandomForestClassifier(criterion='gini',min_samples_leaf=1, max_depth=25,\n",
    "                                           min_samples_split=11, n_estimators=150, oob_score=True,\n",
    "                                          random_state=1,n_jobs=-1)\n",
    "spam_kaggle_rf.fit(SH_train, SH_y)\n",
    "print(\"%.4f\" % spam_kaggle_rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "0.8430\n",
      "205\n",
      "0.8432\n",
      "210\n",
      "0.8436\n",
      "215\n",
      "0.8438\n",
      "220\n",
      "0.8442\n",
      "225\n",
      "0.8432\n",
      "230\n",
      "0.8436\n",
      "235\n",
      "0.8434\n",
      "240\n",
      "0.8438\n",
      "245\n",
      "0.8432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "for i in range(200, 250, 5):\n",
    "    spam_kaggle_rf = RandomForestClassifier(criterion='gini',min_samples_leaf=1, max_depth=25,\n",
    "                                           min_samples_split=11, n_estimators=i, oob_score=True,\n",
    "                                          random_state=1,n_jobs=-1)\n",
    "    spam_kaggle_rf.fit(SH_train, SH_y)\n",
    "    print(i)\n",
    "    print(\"%.4f\" % spam_kaggle_rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_kaggle_rf = RandomForestClassifier(criterion='gini',min_samples_leaf=1, max_depth=25,\n",
    "                                           min_samples_split=11, n_estimators=220, oob_score=False,\n",
    "                                          random_state=1,n_jobs=-1)\n",
    "spam_kaggle_rf.fit(SH_train, SH_y)\n",
    "spam_pred = spam_kaggle_rf.predict(SH_test)\n",
    "results_to_csv(spam_pred, 'spam_kaggle_again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rf = RandomForest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
